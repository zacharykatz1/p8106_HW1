---
title: 'Data Science II (P8106) Homework #1'
author: 'Zachary Katz (UNI: zak2132)'
date: "2/22/2022"
output: 
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 3
  header-includes:
    -\usepackage{fancyhdr}
    -\usepackage{lipsum}
    -\pagestyle{fancy}
    -\fancyhead[R]{\thepage}
    -\fancypagestyle{plain}{\pagestyle{fancy}}

---

## Set-Up and Pre-Processing

In this exercise, we'll predict the sale price of a home based on a set of feature variables. We begin by loading the appropriate libraries and setting our defaults. Then, we'll import our data and eliminate any rows with NAs before providing summary statistics.

```{r setup, include=FALSE}
# Load packages
library(tidyverse)
library(viridis)
library(leaps)
library(corrplot)
library(Hmisc)
library(caret)
library(glmnet)
library(plotmo)
library(pls)

# Set global options for embedding plots and choosing themes
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

```{r data cleaning}
# Load data
training_data = read_csv("./Data/housing_training.csv") %>% 
  janitor::clean_names() 

test_data = read_csv("./Data/housing_test.csv") %>% 
  janitor::clean_names()

# Eliminate rows containing NA entries
training_data = na.omit(training_data)
test_data = na.omit(test_data)
```

```{r data summary}
# Summary statistics of training data
summary(training_data)
skimr::skim(training_data)

# Summary statistics of test data
summary(test_data)
skimr::skim(test_data)

table(sapply(training_data, class))
```

To predict our single continuous, numeric outcome (`sale_price`), we have 25 predictors, including 21 numeric variables (such as `lot_area` and `open_porch_sf`) and 4 factor variables (`overall_qual`, `kitchen_qual`, `fireplace_qu`, and `external_qual`). Our training data has `r nrow(training_data)`observations, while our testing data has `r nrow(test_data)` hold-out observations.

Before developing any models, we create appropriate vectors and matrices for future use:

```{r create matrices}
training_predictors_matrix = model.matrix(sale_price ~ ., training_data)[, -1]
training_outcomes_vector = training_data$sale_price
training_matrix_all = model.matrix( ~ ., training_data)

testing_predictors_matrix = model.matrix(sale_price ~ ., test_data)[, -1]
testing_outcomes_matrix = test_data$sale_price
testing_matrix_all = model.matrix(sale_price ~ ., test_data)
```

## Linear regression

### Correlation Plots and Predictor Subsetting

For completeness, let's check for potential collinearities between predictors in our training data.

```{r correlation plots}
# Correlation plot for all predictors
corrplot(cor(training_predictors_matrix), method = "circle", type = "full")

# Correlation matrix
round(rcorr(training_predictors_matrix)$r, 2)

# Check numerically
round(rcorr(training_predictors_matrix)$r, 2) %>% 
  as.data.frame() %>% 
  pivot_longer(cols = gr_liv_area:year_sold, names_to = "predictor", values_to = "corr") %>% 
  filter(corr < 1) %>% 
  arrange(desc(corr)) %>% 
  head()
```

We have a few collinearities greater than 0.8. We may decide to reduce the number of predictors using a `regsubsets` (best subsets) procedure.

```{r best subset model selection}
regsubsets_obj = regsubsets(sale_price ~ ., data = training_data, method = "exhaustive", nbest = 1)

plot(regsubsets_obj, scale = "bic")

summary(regsubsets_obj)
```

As expected, one primary collinearity was found (between `garage_area` and `garage_cars`). Were we to choose only several predictors to use, they might be, according to our algorithm: `gr_liv_area`, `total_bsmt_sf`, `bsmt_unf_sf`, `year_built`, `overall_qualExcellent`, `overall_qualGood`, `overall_qualVery_Excellent`, `overall_qualVery_Good`, and `lot_area`. 

We can check out what the correlation matrix looks like with only these predictors.

```{r correlations for subset}
# Correlation plot for only best subset predictors
x_best = training_predictors_matrix[, c("gr_liv_area", "total_bsmt_sf", "bsmt_unf_sf", "year_built", "overall_qualExcellent", "overall_qualGood", "overall_qualVery_Excellent", "overall_qualVery_Good", "lot_area")]

corrplot(cor(x_best), method = "circle", type = "full")

# Check numerically
round(rcorr(x_best)$r, 2) %>% 
  as.data.frame()
```

We no longer have any major collinearities using this variable selection procedures. However, for the purposes of this assignment, we'll assume use of all predictors for linear regression.

### Linear regression modeling using `fit_lm`

We can train our model on our training data (including all predictors) using cross-validation. Here are two possible computational approaches.

#### Using `fit_lm`

First, let's fit our model using `fit_lm`.

```{r Linear model fitting fit lm}

set.seed(2132)

# Set cross-validation parameters
control_cv = trainControl(method = "repeatedcv", number = 20, repeats = 5)

# Fit linear model on training data using cross-validation
fit_lm = train(sale_price ~ .,
               preProcess = c("scale", "center"),
               data = training_data,
               method = "lm",
               trControl = control_cv)

# Coefficients of final model
fit_lm$finalModel

# Report cross-validation training RMSE
train_mse_lm = mean(fit_lm$resample$RMSE)
```

We choose the model with the minimum training cross-validation RMSE to apply to our test data. This model's training RMSE after cross-validation is `r train_mse_lm`.

Then, we apply the model to the test data and obtain the test MSE as a measure of accuracy.

```{r Linear model prediction fit lm}

# Predict on test data
predict_lm = predict(fit_lm, newdata = test_data)

# Report test RMSE
rmse_lm = RMSE(predict_lm, test_data$sale_price)

rmse_lm %>% knitr::kable()
```

#### Using `caret`

Notably, we obtain the same linear model coefficients using the `caret` package:

```{r Linear model fitting caret}

set.seed(2132)

# Re-do linear model using glmnet
fit_lm_caret = train(x = training_predictors_matrix, 
                     y = training_outcomes_vector,
                    method = "lm",
                    preProcess = c("center", "scale"),
                    trControl = control_cv)

# Coefficients
round(coef(fit_lm_caret$finalModel), 1)

# Report cross-validation training RMSE
train_mse_lm_caret = mean(fit_lm_caret$resample$RMSE)

train_mse_lm_caret %>% knitr::kable()
```

Once again, after cross-validation, we choose the model with the minimum training RMSE to apply to our test data. This model's training RMSE is `r train_mse_lm_caret`. Then, we'd measure the performance of our best model on the test data, finding the same test RMSE as we found using the first method.

```{r Linear model prediction caret}

# Predict on test data
predict_lm_caret = predict(fit_lm_caret, newdata = testing_matrix_all)

# Report test RMSE
rmse_lm_caret = RMSE(predict_lm_caret, test_data$sale_price)

rmse_lm_caret %>% knitr::kable()
```

There are a number of potential disadvantages with the linear model. First, we know that high correlations between predictions may be problematic, leading the variance of coefficients to increase (hence why we might normally use some subset of predictors, despite the fact that we used all in this case). When we have many predictors, as we have in this data set, we also would prefer many times more observations than we are given; otherwise, we sacrifice degrees of freedom. As shown below, regularized regression may be more appropriate here in order to control variance. For example, lasso regression may yield sparser models. Moreover, linear regression is only limited to linear relationships between predictors and outcome, may be sensitive to outliers, and requires independent data. 

## Lasso model

The Lasso model utilizes an L1 penalty that, at times, forces some of the coefficient estimates to zero with sufficiently large lambda. This provides a useful variable selection process that may improve interpretability of our model.

We'll try four approaches below: two from `glmnet` and two from `caret`. For each package, we'll try either the lambda 1SE rule or the lambda min rule.

### Using `glmnet`

#### Lambda 1SE rule

```{r glmnet lasso training 1se}
cv_lasso_glmnet_1se = cv.glmnet(x = training_predictors_matrix, 
                     y = training_outcomes_vector,
                     standardize = TRUE,
                     alpha = 1,
                     lambda = exp(seq(8, -5, length = 100)))

plot(cv_lasso_glmnet_1se)

# Use 1se rule
cv_lasso_glmnet_1se$lambda.1se %>% knitr::kable()
```

When the 1SE rule is applied, our lambda is `r round(cv_lasso_glmnet_1se$lambda.1se, 1)`. Our optimal coefficients are as follows:

```{r glmnet lasso coefficients 1se}
# Obtain coefficients
lasso_glmnet_coefs_1se = coef(cv_lasso_glmnet_1se, s = "lambda.1se")

# Determine non-zero predictors (included in model)
num_pred = length(which(lasso_glmnet_coefs_1se != 0))
```

Ultimately, we include `r num_pred` predictors (plus the intercept) in our model using this lasso method with 1SE rule for our lambda tuning parameter. The trace plot below shows the the shrinkage in predictor coefficients as lambda increases, with a vertical line included for our chosen lambda 1se value.

```{r glmnet lasso trace plot 1se}
plot(cv_lasso_glmnet_1se$glmnet.fit, "lambda", label = TRUE)
abline(v = 6.686)
```

With a fitted model that uses the lambda 1se, we perform prediction and assess the test error as follows:

```{r glmnet lasso predictions 1se}

# Make predictions using glmnet object
lasso_predict_1se = predict(cv_lasso_glmnet_1se, newx = testing_predictors_matrix, s = "lambda.1se", type = "response")

# Find RMSE test error
rmse_lasso_glmnet_1se = RMSE(lasso_predict_1se, test_data$sale_price)

rmse_lasso_glmnet_1se %>% knitr::kable()

# Alternative assessment of test error
lasso_test_error_1se = assess.glmnet(cv_lasso_glmnet_1se,
                           newx = testing_predictors_matrix,
                           newy = test_data$sale_price)

# Find square root of MSE to get RMSE
lasso_test_error_1se$mse %>% 
  as.data.frame() %>% 
  sqrt() %>% 
  knitr::kable()
```

#### Lambda min rule

```{r glmnet lasso training min}
cv_lasso_glmnet_min = cv.glmnet(x = training_predictors_matrix, 
                     y = training_outcomes_vector,
                     standardize = TRUE,
                     alpha = 1,
                     lambda = exp(seq(8, -5, length = 100)))

plot(cv_lasso_glmnet_min)

# Use 1se rule
cv_lasso_glmnet_min$lambda.min %>% knitr::kable()
```

When the lambda min rule is applied, our lambda is `r round(cv_lasso_glmnet_min$lambda.min, 1)`. Our optimal coefficients are as follows:

```{r glmnet lasso coefficients min}
# Obtain coefficients
lasso_glmnet_coefs_min = coef(cv_lasso_glmnet_min, s = "lambda.min")

# Determine non-zero predictors (included in model)
num_pred_min = length(which(lasso_glmnet_coefs_min != 0))
```

Ultimately, we include `r num_pred` predictors in our model using this lasso method with the min rule for our lambda tuning parameter. The trace plot below shows the the shrinkage in predictor coefficients as lambda increases, with a vertical line included for our chosen lambda min value.

```{r glmnet lasso trace plot min}
plot(cv_lasso_glmnet_min$glmnet.fit, "lambda", label = TRUE)
abline(v = 4.06)
```

With a fitted model that uses the lambda min, we perform prediction and assess the test error as follows:

```{r glmnet lasso predictions min}

# Make predictions using glmnet object
lasso_predict_glmnet_min = predict(cv_lasso_glmnet_min, newx = testing_predictors_matrix, s = "lambda.min", type = "response")

rmse_lasso_glmnet_min = RMSE(lasso_predict_glmnet_min, test_data$sale_price)

rmse_lasso_glmnet_min %>% knitr::kable()
```

### Using `caret`

Rather than using `glmnet`, we can also use the `caret` package.

#### Lambda 1SE rule

```{r caret lasso training 1se}

set.seed(2132)

# Set cross-validation parameters for 1SE
control_cv_1se = trainControl(method = "repeatedcv", number = 20, repeats = 5, selectionFunction = "oneSE")

# Fit model on training data
lasso_caret_fit_1se = train(x = training_predictors_matrix, 
                        y = training_outcomes_vector,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = exp(seq(5, 2, length = 100))),
                  preProcess = c("center", "scale"),
                  trControl = control_cv_1se
)

# Plot RMSE against tuning parameter
plot(lasso_caret_fit_1se, xTrans = log)

# Optimal lambda 1SE
lasso_caret_fit_1se$bestTune$lambda %>% knitr::kable()
```

Our optimal lambda is `r lasso_caret_fit_1se$bestTune$lambda`.

We can also obtain our coefficients in the optimal model, and then make predictions using our test data to determine model performance (RMSE).

```{r caret lasso coefficients 1se}
# Obtain coefficients for final model
coef(lasso_caret_fit_1se$finalModel, lasso_caret_fit_1se$bestTune$lambda)
```

```{r caret lasso prediction 1se}
set.seed(2132)

# Make predictions on test data set
lasso_pred_caret_1se = predict(lasso_caret_fit_1se, newdata = testing_predictors_matrix)

# Find test RMSE
rmse_lasso_caret_1se = RMSE(lasso_pred_caret_1se, test_data$sale_price)

rmse_lasso_caret_1se %>% knitr::kable()
```

#### Lambda min rule

In `caret`, we may alternatively decide to use the more traditional lambda min tuning parameter. We implement this similarly to the above, as follows:

```{r caret lasso training min}

set.seed(2132)

# Fit model on training data
lasso_caret_fit_min = train(x = training_predictors_matrix, 
                        y = training_outcomes_vector,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = 1,
                                         lambda = exp(seq(5, 2, length = 100))),
                  preProcess = c("center", "scale"),
                  trControl = control_cv
)

# Plot RMSE against tuning parameter
plot(lasso_caret_fit_min, xTrans = log)

# Optimal lambda 1SE
lasso_caret_fit_min$bestTune$lambda %>% knitr::kable()
```

Our optimal lambda is `r lasso_caret_fit_min$bestTune$lambda`.

We can also obtain our coefficients in the optimal model, and then make predictions using our test data to determine model performance (RMSE).

```{r caret lasso coefficients min}
# Obtain coefficients for final model
coef(lasso_caret_fit_min$finalModel, lasso_caret_fit_min$bestTune$lambda)
```

```{r caret lasso prediction min}
set.seed(2132)

# Make predictions on test data set
lasso_pred_caret_min = predict(lasso_caret_fit_min, newdata = testing_predictors_matrix)

# Find test RMSE
rmse_lasso_caret_min = RMSE(lasso_pred_caret_min, test_data$sale_price)

rmse_lasso_caret_min %>% knitr::kable()
```

## Elastic net

Elastic net is a more recent method that tends to be more effective when dealing with groups of highly correlated predictors. It includes two types of penalty, drawing on both the lasso and ridge methodologies, permitting us to create an optimal model using two tuning parameters. Here, we show its implementation using the `caret` package.

As always, we first train our model on standardized predictors:

```{r elastic net training}

set.seed(2132)

# Train model
enet_fit = train(x = training_predictors_matrix, y = training_outcomes_vector,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 21),
                                         lambda = exp(seq(-2, 8, length = 50))),
                  preProcess = c("center", "scale"),
                  trControl = control_cv
)

# Optimal tuning parameters
enet_fit$bestTune %>% 
  knitr::kable()
```

Using elastic net, we find that our optimal alpha is `r enet_fit$bestTune$alpha` and our optimal lambda is `r enet_fit$lambda`. This means that our ideal elastic net model is much closer to a ridge model than a lasso model. We can visualize as follows

```{r elastic net viz}
# Rainbow plot settings
myCol = rainbow(25)
myPar = list(superpose.symbol = list(col = myCol),
             superpose.line = list(col = myCol))

# Plot RMSE against lambda, stratified by alpha
plot(enet_fit, par.settings = myPar)
```

And here are the coefficients for our optimal elastic net model:

```{r coefficients elastic net}
# Coefficients for ideal elastic net model
coef(enet_fit$finalModel, enet_fit$bestTune$lambda)
```

Finally, we test our model on the test data set and determine our RMSE accuracy.

```{r elastic net predict}

set.seed(2132)

# Elastic net predictions
enet_pred = predict(enet_fit, newdata = testing_predictors_matrix)

# Elastic net RMSE
rmse_elastic_net = RMSE(enet_pred, test_data$sale_price)

rmse_elastic_net %>% knitr::kable()
```

## Partial least squares model

Again, this can be implemented in more than one package, so we try both here for completeness. The same modeling procedure applies: we fit the model on training data using cross-validation, then once we've selected the optimal model / parameters, we apply it to the new test data.

### Using `pls`

```{r pls using pls}

set.seed(2132)

# Fit PLS model with cross-validation
pls.mod = plsr(sale_price ~ .,
               data = training_data,
               scale = TRUE,
               validation = "CV")

# Summary of PLS model fit, including % variance explained and RMSEP
summary(pls.mod) %>% knitr::kable()

# Plot MSEP against number of components
validationplot(pls.mod, val.type = "MSEP", legendpos = "topright")

# Find cross-validation RMSE error (training data) only
cv.mse = RMSEP(pls.mod)

# How many components?
ncomp.cv = which.min(cv.mse$val[1, , ])-1
```

Using the `pls` package, we obtain a test error of `r pls_rmse` (RMSE), with `r ncomp.cv` components included in the model. We use this optimal model to make predictions on the test data and find test error.

```{r pls predict using pls}
pred.pls = predict(pls.mod, newdata = test_data, ncomp = ncomp.cv)

RMSE(pred.pls, test_data$sale_price) %>% 
  knitr::kable()
```

### Using `caret`

Alternatively, we can implement the partial least squares method similarly in `caret`.

```{r pls using caret}

set.seed(2132)

# Set cross-validation parameters again
control_cv_best = trainControl(method = "repeatedcv", number = 20, repeats = 5, selectionFunction = "best")

# Train model on training data using cross-validation
pls_fit_caret = train(x = training_predictors_matrix,
                      y = training_outcomes_vector,
                      method = "pls",
                      tuneGrid = data.frame(ncomp = 1:39),
                      trControl = control_cv_best,
                      preProcess = c("center", "scale"))

# Plot training RMSE against # of components
ggplot(pls_fit_caret, highlight = TRUE) + theme_bw()

# Apply optimal model to test data
pls_pred_caret = predict(pls_fit_caret, newdata = testing_predictors_matrix)

# Find test error
caret_pls_rmse = RMSE(pls_pred_caret, test_data$sale_price)
```

Using the `caret` package, we obtain a test error of `r caret_pls_rmse` (RMSE) when we apply the optimal model, which has 15 components, to our test data set.

## Model comparison

Finally, we use a resampling method to check how our final models perform on RMSE.

```{r model comparison}
set.seed(2132)

# Resampling testing
resamp = resamples(list(lm = fit_lm_caret, lasso_1se = lasso_caret_fit_1se, lasso_min = lasso_caret_fit_min, enet = enet_fit, pls = pls_fit_caret))

# Summary statistics on resampling
summary(resamp)

# Box plots of resampling for RMSE
bwplot(resamp, metric = "RMSE")
```

Based on this, we would likely use the elastic net model to predict our response because its mean RMSE over cross-validation trials is lower than the mean RMSE found for the other methods. That said, all of our methods actually give us relatively similar RMSE estimates.